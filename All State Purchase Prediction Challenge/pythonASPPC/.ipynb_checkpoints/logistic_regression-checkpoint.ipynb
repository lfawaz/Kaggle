{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lfawaz/miniconda2/envs/python27/lib/python2.7/site-packages/ipykernel/__main__.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import dateparser\n",
    "from  sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "\n",
    "def load_clean_data():\n",
    "\n",
    "    train = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "    print\"load data...\"\n",
    "    model_data = train\n",
    "    ##create hour\n",
    "    model_data['hour'] = model_data['time'].apply(lambda x: x.split(':')[0])\n",
    "    ##create minute\n",
    "    model_data['minute'] = model_data['time'].apply(lambda x: x.split(':')[1])\n",
    "\n",
    "\n",
    "    ##replace car value, risk_fact,C_Previous,duration_previous null with -1\n",
    "    print\"removing nulls...\"\n",
    "    null_columns = ['car_value','risk_factor','C_previous','duration_previous']\n",
    "    for col in null_columns:\n",
    "        model_data[col] = model_data[col].apply(lambda x: -1 if pd.isnull(x) else x)\n",
    "\n",
    "    ############################################################################################   \n",
    "    ##implement is_last column this determines what was the last record the customer looked at##\n",
    "    ############################################################################################\n",
    "    #Select first two columns for faster processing\n",
    "    is_last_data = model_data[['customer_ID','shopping_pt']]\n",
    "    #Set an empty column to is_last\n",
    "    is_last_data['is_last'] = 0\n",
    "\n",
    "    #convert the Pandas frame work to numpy because it is faster to loop through\n",
    "    np_is_last_data = np.asarray(is_last_data)\n",
    "    print \"adding is_last column ...\"\n",
    "    #create a column to indicate if this was the last viewed record\n",
    "    for i in range(len(np_is_last_data)):\n",
    "        if np_is_last_data[i][1] == 1:\n",
    "            np_is_last_data[i - 1][2] = 1\n",
    "\n",
    "    #create the data frame with the is_last column\n",
    "    is_last_data = pd.DataFrame(np_is_last_data, columns=is_last_data.columns.values)\n",
    "\n",
    "\n",
    "    ######################################################################\n",
    "    #create a flag to determine if the record was the finally sold record#\n",
    "    ######################################################################\n",
    "\n",
    "    #outer join data with subset of purchases on all the product items\n",
    "    print\"adding is_final column -predictor-\"\n",
    "    #select the purchased record\n",
    "    sold_records_only = model_data[['customer_ID','shopping_pt','A','B','C','D','E','F','G']][(model_data.record_type == 1)]\n",
    "    is_final_merge = pd.merge(model_data[['customer_ID','shopping_pt','A','B','C','D','E','F','G']],sold_records_only,on=['customer_ID','A','B','C','D','E','F','G'], how='outer')\n",
    "\n",
    "    #lamdba function if the value of shopping_pt_y is null since it is outer join then the production was not \n",
    "    #purchased otherwise it was eventually purchase, we will use this column as our predictor\n",
    "    is_final_merge['is_final'] = is_final_merge['shopping_pt_y'].apply(lambda x: 0 if pd.isnull(x) else 1)\n",
    "    is_final_merge.rename(columns={'shopping_pt_x':'shopping_pt'}, inplace=True)\n",
    "    is_final_data = is_final_merge[['customer_ID','shopping_pt','is_final']]\n",
    "\n",
    "\n",
    "    ###################################################################\n",
    "    #create a column to indicate how many times this record was viewed#\n",
    "    ###################################################################\n",
    "    print\"adding viewed total column...\"\n",
    "    #Group by the customer and the product\n",
    "    total_viewed_group_by = model_data.groupby(['customer_ID','A','B','C','D','E','F','G']).size().reset_index()\n",
    "    #relabel the last column as total views\n",
    "    total_viewed_group_by.columns = ['customer_ID', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'total_viewed']\n",
    "\n",
    "    #add total_viewed column to original dataset\n",
    "    total_viewed_data = pd.merge(model_data[['customer_ID','shopping_pt','A','B','C','D','E','F','G']],total_viewed_group_by,on=['customer_ID','A','B','C','D','E','F','G'])[['customer_ID','shopping_pt','total_viewed']]\n",
    "    print\"converting states to floats...\"\n",
    "    ##convert state to floats to allow for categorical data processing\n",
    "    state_dummies = pd.get_dummies(model_data['state'])\n",
    "    state_data = model_data.join(state_dummies)[[u'customer_ID', u'shopping_pt', u'AL', u'AR',\\\n",
    "           u'CO', u'CT', u'DC', u'DE', u'FL', u'GA', u'IA', u'ID', u'IN', u'KS',\\\n",
    "           u'KY', u'MD', u'ME', u'MO', u'MS', u'MT', u'ND', u'NE', u'NH', u'NM',\\\n",
    "           u'NV', u'NY', u'OH', u'OK', u'OR', u'PA', u'RI', u'SD', u'TN', u'UT',\\\n",
    "           u'WA', u'WI', u'WV', u'WY']]\n",
    "    print\"converting car values to floats...\"\n",
    "    ##convert car values to floats to allow for categorical data processing\n",
    "    car_value_dummies = pd.get_dummies(model_data['car_value'])\n",
    "    car_value_data = model_data.join(car_value_dummies)[['customer_ID','shopping_pt',u'a', u'b',\\\n",
    "                           u'c',                 u'd',                 u'e',\\\n",
    "                           u'f',                 u'g',                 u'h',\\\n",
    "                           u'i']]\n",
    "\n",
    "    original_model_data = model_data[['customer_ID','shopping_pt','day','location','group_size','homeowner','car_age',\\\n",
    "                                  'risk_factor','age_oldest','age_youngest','married_couple',\\\n",
    "                                  'C_previous','duration_previous', 'cost','hour','minute']][(model_data.record_type != 1)]\n",
    "    print\"merging all datasets...\"\n",
    "    all_new_data = pd.merge(car_value_data,\\\n",
    "                            pd.merge(state_data,\\\n",
    "                                     pd.merge(total_viewed_data,\\\n",
    "                                              pd.merge(is_last_data,is_final_data, \\\n",
    "                                                       on=['customer_ID','shopping_pt']), \\\n",
    "                                                          on=['customer_ID','shopping_pt']) ,\\\n",
    "                                                             on=['customer_ID','shopping_pt']),\\\n",
    "                                                                on=['customer_ID','shopping_pt'])\n",
    "    print\"creating final model...\"\n",
    "    final_model_data = pd.merge(original_model_data,all_new_data,on=['customer_ID','shopping_pt'],how='inner')\n",
    "\n",
    "    X = np.asarray(final_model_data.ix[:, final_model_data.columns.difference(['customer_ID','shopping_pt','is_final'])])\n",
    "\n",
    "    y = np.asarray(final_model_data.is_final)\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "X,y = load_clean_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.33,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_model = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_logmodel = ml_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = ml_logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i] == y_test[i]:\n",
    "        l.append(1)\n",
    "    else:\n",
    "        l.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(l,columns=['is_predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = results.groupby('is_predicted').size()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sknn.mlp import Classifier, Layer\n",
    "\n",
    "nn = Classifier(\n",
    "    layers=[\n",
    "        Layer(\"Rectifier\", units=100),\n",
    "        Layer(\"Softmax\")],\n",
    "    learning_rate=0.02,\n",
    "    n_iter=10)\n",
    "nn.fit(X_train, y_train)\n",
    "\n",
    "y_valid = nn.predict(X_train)\n",
    "\n",
    "score = nn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
